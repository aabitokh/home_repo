{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.4"
    },
    "colab": {
      "name": "spark.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlKvIWD3kXFg",
        "outputId": "87f0fdaf-233d-4d9d-b871-6941e58d067d"
      },
      "source": [
        "!pip install pyspark "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 70kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 19.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=e28d5be25c5df64403c9a2f8836965c908701dfaec6a676c389165f95531b1aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcPxROIkihW1",
        "outputId": "1b23c7fa-c5e5-40f9-e2a5-d01f1301d2f4"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import pandas as pd \n",
        "from pyspark.sql.functions import year, month\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL24HKNaihW3"
      },
      "source": [
        "sc = SparkContext.getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvPXHs3dihW3",
        "outputId": "f270bae9-09cd-46d5-9b80-10157336561e"
      },
      "source": [
        "type(sc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.context.SparkContext"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbyziycnihW4",
        "outputId": "19c9c0f1-5b78-4b45-c001-3710dc663f2d"
      },
      "source": [
        "help(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on SparkContext in module pyspark.context object:\n",
            "\n",
            "class SparkContext(builtins.object)\n",
            " |  SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
            " |  \n",
            " |  Main entry point for Spark functionality. A SparkContext represents the\n",
            " |  connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
            " |  broadcast variables on that cluster.\n",
            " |  \n",
            " |  .. note:: Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
            " |      the active :class:`SparkContext` before creating a new one.\n",
            " |  \n",
            " |  .. note:: :class:`SparkContext` instance is not supported to share across multiple\n",
            " |      processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
            " |      Use threads instead for concurrent processing purpose.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __enter__(self)\n",
            " |      Enable 'with SparkContext(...) as sc: app(sc)' syntax.\n",
            " |  \n",
            " |  __exit__(self, type, value, trace)\n",
            " |      Enable 'with SparkContext(...) as sc: app' syntax.\n",
            " |      \n",
            " |      Specifically stop the context on exit of the with block.\n",
            " |  \n",
            " |  __getnewargs__(self)\n",
            " |  \n",
            " |  __init__(self, master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=<class 'pyspark.profiler.BasicProfiler'>)\n",
            " |      Create a new SparkContext. At least the master and app name should be set,\n",
            " |      either through the named parameters here or through `conf`.\n",
            " |      \n",
            " |      :param master: Cluster URL to connect to\n",
            " |             (e.g. mesos://host:port, spark://host:port, local[4]).\n",
            " |      :param appName: A name for your job, to display on the cluster web UI.\n",
            " |      :param sparkHome: Location where Spark is installed on cluster nodes.\n",
            " |      :param pyFiles: Collection of .zip or .py files to send to the cluster\n",
            " |             and add to PYTHONPATH.  These can be paths on the local file\n",
            " |             system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
            " |      :param environment: A dictionary of environment variables to set on\n",
            " |             worker nodes.\n",
            " |      :param batchSize: The number of Python objects represented as a single\n",
            " |             Java object. Set 1 to disable batching, 0 to automatically choose\n",
            " |             the batch size based on object sizes, or -1 to use an unlimited\n",
            " |             batch size\n",
            " |      :param serializer: The serializer for RDDs.\n",
            " |      :param conf: A :class:`SparkConf` object setting Spark properties.\n",
            " |      :param gateway: Use an existing gateway and JVM, otherwise a new JVM\n",
            " |             will be instantiated.\n",
            " |      :param jsc: The JavaSparkContext instance (optional).\n",
            " |      :param profiler_cls: A class of custom Profiler used to do profiling\n",
            " |             (default is pyspark.profiler.BasicProfiler).\n",
            " |      \n",
            " |      \n",
            " |      >>> from pyspark.context import SparkContext\n",
            " |      >>> sc = SparkContext('local', 'test')\n",
            " |      \n",
            " |      >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |      Traceback (most recent call last):\n",
            " |          ...\n",
            " |      ValueError:...\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  accumulator(self, value, accum_param=None)\n",
            " |      Create an :class:`Accumulator` with the given initial value, using a given\n",
            " |      :class:`AccumulatorParam` helper object to define how to add values of the\n",
            " |      data type if provided. Default AccumulatorParams are used for integers\n",
            " |      and floating-point numbers if you do not provide one. For other types,\n",
            " |      a custom AccumulatorParam can be used.\n",
            " |  \n",
            " |  addFile(self, path, recursive=False)\n",
            " |      Add a file to be downloaded with this Spark job on every node.\n",
            " |      The `path` passed can be either a local file, a file in HDFS\n",
            " |      (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
            " |      FTP URI.\n",
            " |      \n",
            " |      To access the file in Spark jobs, use :meth:`SparkFiles.get` with the\n",
            " |      filename to find its download location.\n",
            " |      \n",
            " |      A directory can be given if the recursive option is set to True.\n",
            " |      Currently directories are only supported for Hadoop-supported filesystems.\n",
            " |      \n",
            " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
            " |      \n",
            " |      >>> from pyspark import SparkFiles\n",
            " |      >>> path = os.path.join(tempdir, \"test.txt\")\n",
            " |      >>> with open(path, \"w\") as testFile:\n",
            " |      ...    _ = testFile.write(\"100\")\n",
            " |      >>> sc.addFile(path)\n",
            " |      >>> def func(iterator):\n",
            " |      ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
            " |      ...        fileVal = int(testFile.readline())\n",
            " |      ...        return [x * fileVal for x in iterator]\n",
            " |      >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
            " |      [100, 200, 300, 400]\n",
            " |  \n",
            " |  addPyFile(self, path)\n",
            " |      Add a .py or .zip dependency for all tasks to be executed on this\n",
            " |      SparkContext in the future.  The `path` passed can be either a local\n",
            " |      file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
            " |      HTTP, HTTPS or FTP URI.\n",
            " |      \n",
            " |      .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\n",
            " |  \n",
            " |  binaryFiles(self, path, minPartitions=None)\n",
            " |      Read a directory of binary files from HDFS, a local file system\n",
            " |      (available on all nodes), or any Hadoop-supported file system URI\n",
            " |      as a byte array. Each file is read as a single record and returned\n",
            " |      in a key-value pair, where the key is the path of each file, the\n",
            " |      value is the content of each file.\n",
            " |      \n",
            " |      .. note:: Small files are preferred, large file is also allowable, but\n",
            " |          may cause bad performance.\n",
            " |  \n",
            " |  binaryRecords(self, path, recordLength)\n",
            " |      Load data from a flat binary file, assuming each record is a set of numbers\n",
            " |      with the specified numerical format (see ByteBuffer), and the number of\n",
            " |      bytes per record is constant.\n",
            " |      \n",
            " |      :param path: Directory to the input data files\n",
            " |      :param recordLength: The length at which to split the records\n",
            " |  \n",
            " |  broadcast(self, value)\n",
            " |      Broadcast a read-only variable to the cluster, returning a :class:`Broadcast`\n",
            " |      object for reading it in distributed functions. The variable will\n",
            " |      be sent to each cluster only once.\n",
            " |  \n",
            " |  cancelAllJobs(self)\n",
            " |      Cancel all jobs that have been scheduled or are running.\n",
            " |  \n",
            " |  cancelJobGroup(self, groupId)\n",
            " |      Cancel active jobs for the specified group. See :meth:`SparkContext.setJobGroup`.\n",
            " |      for more information.\n",
            " |  \n",
            " |  dump_profiles(self, path)\n",
            " |      Dump the profile stats into directory `path`\n",
            " |  \n",
            " |  emptyRDD(self)\n",
            " |      Create an RDD that has no partitions or elements.\n",
            " |  \n",
            " |  getConf(self)\n",
            " |  \n",
            " |  getLocalProperty(self, key)\n",
            " |      Get a local property set in this thread, or null if it is missing. See\n",
            " |      :meth:`setLocalProperty`.\n",
            " |  \n",
            " |  hadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
            " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
            " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
            " |      The mechanism is the same as for sc.sequenceFile.\n",
            " |      \n",
            " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
            " |      Configuration in Java.\n",
            " |      \n",
            " |      :param path: path to Hadoop file\n",
            " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
            " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
            " |      :param keyClass: fully qualified classname of key Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      :param valueClass: fully qualified classname of value Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      :param keyConverter: (None by default)\n",
            " |      :param valueConverter: (None by default)\n",
            " |      :param conf: Hadoop configuration, passed in as a dict\n",
            " |             (None by default)\n",
            " |      :param batchSize: The number of Python objects represented as a single\n",
            " |             Java object. (default 0, choose batchSize automatically)\n",
            " |  \n",
            " |  hadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
            " |      Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
            " |      Hadoop configuration, which is passed in as a Python dict.\n",
            " |      This will be converted into a Configuration in Java.\n",
            " |      The mechanism is the same as for sc.sequenceFile.\n",
            " |      \n",
            " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
            " |             (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
            " |      :param keyClass: fully qualified classname of key Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      :param valueClass: fully qualified classname of value Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      :param keyConverter: (None by default)\n",
            " |      :param valueConverter: (None by default)\n",
            " |      :param conf: Hadoop configuration, passed in as a dict\n",
            " |             (None by default)\n",
            " |      :param batchSize: The number of Python objects represented as a single\n",
            " |             Java object. (default 0, choose batchSize automatically)\n",
            " |  \n",
            " |  newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
            " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
            " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
            " |      The mechanism is the same as for sc.sequenceFile.\n",
            " |      \n",
            " |      A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
            " |      Configuration in Java\n",
            " |      \n",
            " |      :param path: path to Hadoop file\n",
            " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
            " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
            " |      :param keyClass: fully qualified classname of key Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      :param valueClass: fully qualified classname of value Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      :param keyConverter: (None by default)\n",
            " |      :param valueConverter: (None by default)\n",
            " |      :param conf: Hadoop configuration, passed in as a dict\n",
            " |             (None by default)\n",
            " |      :param batchSize: The number of Python objects represented as a single\n",
            " |             Java object. (default 0, choose batchSize automatically)\n",
            " |  \n",
            " |  newAPIHadoopRDD(self, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)\n",
            " |      Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
            " |      Hadoop configuration, which is passed in as a Python dict.\n",
            " |      This will be converted into a Configuration in Java.\n",
            " |      The mechanism is the same as for sc.sequenceFile.\n",
            " |      \n",
            " |      :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
            " |             (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
            " |      :param keyClass: fully qualified classname of key Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      :param valueClass: fully qualified classname of value Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      :param keyConverter: (None by default)\n",
            " |      :param valueConverter: (None by default)\n",
            " |      :param conf: Hadoop configuration, passed in as a dict\n",
            " |             (None by default)\n",
            " |      :param batchSize: The number of Python objects represented as a single\n",
            " |             Java object. (default 0, choose batchSize automatically)\n",
            " |  \n",
            " |  parallelize(self, c, numSlices=None)\n",
            " |      Distribute a local Python collection to form an RDD. Using xrange\n",
            " |      is recommended if the input represents a range for performance.\n",
            " |      \n",
            " |      >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
            " |      [[0], [2], [3], [4], [6]]\n",
            " |      >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
            " |      [[], [0], [], [2], [4]]\n",
            " |  \n",
            " |  pickleFile(self, name, minPartitions=None)\n",
            " |      Load an RDD previously saved using :meth:`RDD.saveAsPickleFile` method.\n",
            " |      \n",
            " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
            " |      >>> tmpFile.close()\n",
            " |      >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
            " |      >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
            " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " |  \n",
            " |  range(self, start, end=None, step=1, numSlices=None)\n",
            " |      Create a new RDD of int containing elements from `start` to `end`\n",
            " |      (exclusive), increased by `step` every element. Can be called the same\n",
            " |      way as python's built-in range() function. If called with a single argument,\n",
            " |      the argument is interpreted as `end`, and `start` is set to 0.\n",
            " |      \n",
            " |      :param start: the start value\n",
            " |      :param end: the end value (exclusive)\n",
            " |      :param step: the incremental step (default: 1)\n",
            " |      :param numSlices: the number of partitions of the new RDD\n",
            " |      :return: An RDD of int\n",
            " |      \n",
            " |      >>> sc.range(5).collect()\n",
            " |      [0, 1, 2, 3, 4]\n",
            " |      >>> sc.range(2, 4).collect()\n",
            " |      [2, 3]\n",
            " |      >>> sc.range(1, 7, 2).collect()\n",
            " |      [1, 3, 5]\n",
            " |  \n",
            " |  runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)\n",
            " |      Executes the given partitionFunc on the specified set of partitions,\n",
            " |      returning the result as an array of elements.\n",
            " |      \n",
            " |      If 'partitions' is not specified, this will run over all partitions.\n",
            " |      \n",
            " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
            " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
            " |      [0, 1, 4, 9, 16, 25]\n",
            " |      \n",
            " |      >>> myRDD = sc.parallelize(range(6), 3)\n",
            " |      >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
            " |      [0, 1, 16, 25]\n",
            " |  \n",
            " |  sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)\n",
            " |      Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
            " |      a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
            " |      The mechanism is as follows:\n",
            " |      \n",
            " |          1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
            " |             and value Writable classes\n",
            " |          2. Serialization is attempted via Pyrolite pickling\n",
            " |          3. If this fails, the fallback is to call 'toString' on each key and value\n",
            " |          4. :class:`PickleSerializer` is used to deserialize pickled objects on the Python side\n",
            " |      \n",
            " |      :param path: path to sequncefile\n",
            " |      :param keyClass: fully qualified classname of key Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.Text\")\n",
            " |      :param valueClass: fully qualified classname of value Writable class\n",
            " |             (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
            " |      :param keyConverter:\n",
            " |      :param valueConverter:\n",
            " |      :param minSplits: minimum splits in dataset\n",
            " |             (default min(2, sc.defaultParallelism))\n",
            " |      :param batchSize: The number of Python objects represented as a single\n",
            " |             Java object. (default 0, choose batchSize automatically)\n",
            " |  \n",
            " |  setCheckpointDir(self, dirName)\n",
            " |      Set the directory under which RDDs are going to be checkpointed. The\n",
            " |      directory must be an HDFS path if running on a cluster.\n",
            " |  \n",
            " |  setJobDescription(self, value)\n",
            " |      Set a human readable description of the current job.\n",
            " |      \n",
            " |      .. note:: Currently, setting a job description (set to local properties) with multiple\n",
            " |          threads does not properly work. Internally threads on PVM and JVM are not synced,\n",
            " |          and JVM thread can be reused for multiple threads on PVM, which fails to isolate\n",
            " |          local properties for each thread on PVM.\n",
            " |      \n",
            " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
            " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
            " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
            " |          local properties.\n",
            " |      \n",
            " |          To work around this, you should manually copy and set the local\n",
            " |          properties from the parent thread to the child thread when you create another thread.\n",
            " |  \n",
            " |  setJobGroup(self, groupId, description, interruptOnCancel=False)\n",
            " |      Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
            " |      different value or cleared.\n",
            " |      \n",
            " |      Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
            " |      Application programmers can use this method to group all those jobs together and give a\n",
            " |      group description. Once set, the Spark web UI will associate such jobs with this group.\n",
            " |      \n",
            " |      The application can use :meth:`SparkContext.cancelJobGroup` to cancel all\n",
            " |      running jobs in this group.\n",
            " |      \n",
            " |      >>> import threading\n",
            " |      >>> from time import sleep\n",
            " |      >>> result = \"Not Set\"\n",
            " |      >>> lock = threading.Lock()\n",
            " |      >>> def map_func(x):\n",
            " |      ...     sleep(100)\n",
            " |      ...     raise Exception(\"Task should have been cancelled\")\n",
            " |      >>> def start_job(x):\n",
            " |      ...     global result\n",
            " |      ...     try:\n",
            " |      ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
            " |      ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
            " |      ...     except Exception as e:\n",
            " |      ...         result = \"Cancelled\"\n",
            " |      ...     lock.release()\n",
            " |      >>> def stop_job():\n",
            " |      ...     sleep(5)\n",
            " |      ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
            " |      >>> suppress = lock.acquire()\n",
            " |      >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\n",
            " |      >>> suppress = threading.Thread(target=stop_job).start()\n",
            " |      >>> suppress = lock.acquire()\n",
            " |      >>> print(result)\n",
            " |      Cancelled\n",
            " |      \n",
            " |      If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
            " |      in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
            " |      ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
            " |      to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
            " |      \n",
            " |      .. note:: Currently, setting a group ID (set to local properties) with multiple threads\n",
            " |          does not properly work. Internally threads on PVM and JVM are not synced, and JVM\n",
            " |          thread can be reused for multiple threads on PVM, which fails to isolate local\n",
            " |          properties for each thread on PVM.\n",
            " |      \n",
            " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
            " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
            " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
            " |          local properties.\n",
            " |      \n",
            " |          To work around this, you should manually copy and set the local\n",
            " |          properties from the parent thread to the child thread when you create another thread.\n",
            " |  \n",
            " |  setLocalProperty(self, key, value)\n",
            " |      Set a local property that affects jobs submitted from this thread, such as the\n",
            " |      Spark fair scheduler pool.\n",
            " |      \n",
            " |      .. note:: Currently, setting a local property with multiple threads does not properly work.\n",
            " |          Internally threads on PVM and JVM are not synced, and JVM thread\n",
            " |          can be reused for multiple threads on PVM, which fails to isolate local properties\n",
            " |          for each thread on PVM.\n",
            " |      \n",
            " |          To work around this, you can set `PYSPARK_PIN_THREAD` to\n",
            " |          `'true'` (see SPARK-22340). However, note that it cannot inherit the local properties\n",
            " |          from the parent thread although it isolates each thread on PVM and JVM with its own\n",
            " |          local properties.\n",
            " |      \n",
            " |          To work around this, you should manually copy and set the local\n",
            " |          properties from the parent thread to the child thread when you create another thread.\n",
            " |  \n",
            " |  setLogLevel(self, logLevel)\n",
            " |      Control our logLevel. This overrides any user-defined log settings.\n",
            " |      Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN\n",
            " |  \n",
            " |  show_profiles(self)\n",
            " |      Print the profile stats to stdout\n",
            " |  \n",
            " |  sparkUser(self)\n",
            " |      Get SPARK_USER for user who is running SparkContext.\n",
            " |  \n",
            " |  statusTracker(self)\n",
            " |      Return :class:`StatusTracker` object\n",
            " |  \n",
            " |  stop(self)\n",
            " |      Shut down the SparkContext.\n",
            " |  \n",
            " |  textFile(self, name, minPartitions=None, use_unicode=True)\n",
            " |      Read a text file from HDFS, a local file system (available on all\n",
            " |      nodes), or any Hadoop-supported file system URI, and return it as an\n",
            " |      RDD of Strings.\n",
            " |      The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
            " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
            " |      Spark 1.2)\n",
            " |      \n",
            " |      >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
            " |      >>> with open(path, \"w\") as testFile:\n",
            " |      ...    _ = testFile.write(\"Hello world!\")\n",
            " |      >>> textFile = sc.textFile(path)\n",
            " |      >>> textFile.collect()\n",
            " |      ['Hello world!']\n",
            " |  \n",
            " |  union(self, rdds)\n",
            " |      Build the union of a list of RDDs.\n",
            " |      \n",
            " |      This supports unions() of RDDs with different serialized formats,\n",
            " |      although this forces them to be reserialized using the default\n",
            " |      serializer:\n",
            " |      \n",
            " |      >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
            " |      >>> with open(path, \"w\") as testFile:\n",
            " |      ...    _ = testFile.write(\"Hello\")\n",
            " |      >>> textFile = sc.textFile(path)\n",
            " |      >>> textFile.collect()\n",
            " |      ['Hello']\n",
            " |      >>> parallelized = sc.parallelize([\"World!\"])\n",
            " |      >>> sorted(sc.union([textFile, parallelized]).collect())\n",
            " |      ['Hello', 'World!']\n",
            " |  \n",
            " |  wholeTextFiles(self, path, minPartitions=None, use_unicode=True)\n",
            " |      Read a directory of text files from HDFS, a local file system\n",
            " |      (available on all nodes), or any  Hadoop-supported file system\n",
            " |      URI. Each file is read as a single record and returned in a\n",
            " |      key-value pair, where the key is the path of each file, the\n",
            " |      value is the content of each file.\n",
            " |      The text files must be encoded as UTF-8.\n",
            " |      \n",
            " |      If use_unicode is False, the strings will be kept as `str` (encoding\n",
            " |      as `utf-8`), which is faster and smaller than unicode. (Added in\n",
            " |      Spark 1.2)\n",
            " |      \n",
            " |      For example, if you have the following files:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          hdfs://a-hdfs-path/part-00000\n",
            " |          hdfs://a-hdfs-path/part-00001\n",
            " |          ...\n",
            " |          hdfs://a-hdfs-path/part-nnnnn\n",
            " |      \n",
            " |      Do ``rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")``,\n",
            " |      then ``rdd`` contains:\n",
            " |      \n",
            " |      .. code-block:: text\n",
            " |      \n",
            " |          (a-hdfs-path/part-00000, its content)\n",
            " |          (a-hdfs-path/part-00001, its content)\n",
            " |          ...\n",
            " |          (a-hdfs-path/part-nnnnn, its content)\n",
            " |      \n",
            " |      .. note:: Small files are preferred, as each file will be loaded\n",
            " |          fully in memory.\n",
            " |      \n",
            " |      >>> dirPath = os.path.join(tempdir, \"files\")\n",
            " |      >>> os.mkdir(dirPath)\n",
            " |      >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
            " |      ...    _ = file1.write(\"1\")\n",
            " |      >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
            " |      ...    _ = file2.write(\"2\")\n",
            " |      >>> textFiles = sc.wholeTextFiles(dirPath)\n",
            " |      >>> sorted(textFiles.collect())\n",
            " |      [('.../1.txt', '1'), ('.../2.txt', '2')]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  getOrCreate(conf=None) from builtins.type\n",
            " |      Get or instantiate a SparkContext and register it as a singleton object.\n",
            " |      \n",
            " |      :param conf: SparkConf (optional)\n",
            " |  \n",
            " |  setSystemProperty(key, value) from builtins.type\n",
            " |      Set a Java system property, such as spark.executor.memory. This must\n",
            " |      must be invoked before instantiating SparkContext.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  applicationId\n",
            " |      A unique identifier for the Spark application.\n",
            " |      Its format depends on the scheduler implementation.\n",
            " |      \n",
            " |      * in case of local spark app something like 'local-1433865536131'\n",
            " |      * in case of YARN something like 'application_1433865536131_34483'\n",
            " |      \n",
            " |      >>> sc.applicationId  # doctest: +ELLIPSIS\n",
            " |      'local-...'\n",
            " |  \n",
            " |  defaultMinPartitions\n",
            " |      Default min number of partitions for Hadoop RDDs when not given by user\n",
            " |  \n",
            " |  defaultParallelism\n",
            " |      Default level of parallelism to use when not given by user (e.g. for\n",
            " |      reduce tasks)\n",
            " |  \n",
            " |  resources\n",
            " |  \n",
            " |  startTime\n",
            " |      Return the epoch time when the Spark Context was started.\n",
            " |  \n",
            " |  uiWebUrl\n",
            " |      Return the URL of the SparkUI instance started by this SparkContext\n",
            " |  \n",
            " |  version\n",
            " |      The version of Spark on which this application is running.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  PACKAGE_EXTENSIONS = ('.zip', '.egg', '.jar')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOZboLWAihW4",
        "outputId": "938f6ab8-59b9-410b-fd48-ac521187ffd6"
      },
      "source": [
        "dir(sc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PACKAGE_EXTENSIONS',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__enter__',\n",
              " '__eq__',\n",
              " '__exit__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getnewargs__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_accumulatorServer',\n",
              " '_active_spark_context',\n",
              " '_assert_on_driver',\n",
              " '_batchSize',\n",
              " '_callsite',\n",
              " '_checkpointFile',\n",
              " '_conf',\n",
              " '_dictToJavaMap',\n",
              " '_do_init',\n",
              " '_encryption_enabled',\n",
              " '_ensure_initialized',\n",
              " '_gateway',\n",
              " '_getJavaStorageLevel',\n",
              " '_initialize_context',\n",
              " '_javaAccumulator',\n",
              " '_jsc',\n",
              " '_jvm',\n",
              " '_lock',\n",
              " '_next_accum_id',\n",
              " '_pickled_broadcast_vars',\n",
              " '_python_includes',\n",
              " '_repr_html_',\n",
              " '_serialize_to_jvm',\n",
              " '_temp_dir',\n",
              " '_unbatched_serializer',\n",
              " 'accumulator',\n",
              " 'addFile',\n",
              " 'addPyFile',\n",
              " 'appName',\n",
              " 'applicationId',\n",
              " 'binaryFiles',\n",
              " 'binaryRecords',\n",
              " 'broadcast',\n",
              " 'cancelAllJobs',\n",
              " 'cancelJobGroup',\n",
              " 'defaultMinPartitions',\n",
              " 'defaultParallelism',\n",
              " 'dump_profiles',\n",
              " 'emptyRDD',\n",
              " 'environment',\n",
              " 'getCheckpointDir',\n",
              " 'getConf',\n",
              " 'getLocalProperty',\n",
              " 'getOrCreate',\n",
              " 'hadoopFile',\n",
              " 'hadoopRDD',\n",
              " 'master',\n",
              " 'newAPIHadoopFile',\n",
              " 'newAPIHadoopRDD',\n",
              " 'parallelize',\n",
              " 'pickleFile',\n",
              " 'profiler_collector',\n",
              " 'pythonExec',\n",
              " 'pythonVer',\n",
              " 'range',\n",
              " 'resources',\n",
              " 'runJob',\n",
              " 'sequenceFile',\n",
              " 'serializer',\n",
              " 'setCheckpointDir',\n",
              " 'setJobDescription',\n",
              " 'setJobGroup',\n",
              " 'setLocalProperty',\n",
              " 'setLogLevel',\n",
              " 'setSystemProperty',\n",
              " 'show_profiles',\n",
              " 'sparkHome',\n",
              " 'sparkUser',\n",
              " 'startTime',\n",
              " 'statusTracker',\n",
              " 'stop',\n",
              " 'textFile',\n",
              " 'uiWebUrl',\n",
              " 'union',\n",
              " 'version',\n",
              " 'wholeTextFiles']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noajl8Q1ow64",
        "outputId": "1f0c16ef-f0d6-4e01-827f-d03af719382b"
      },
      "source": [
        "sqlContext = SQLContext(sc)\n",
        "li = [('Alice', 1)]\n",
        "df = sqlContext.createDataFrame(li, ['name', 'age'])\n",
        "df.collect()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='Alice', age=1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y8vcR5OpTg9"
      },
      "source": [
        "path_to_data = ''"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY59W7RIihW5"
      },
      "source": [
        "tesla_rdd = sc.textFile(path_to_data + '/TSLA.csv')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3qqWElkihW5",
        "outputId": "36db9843-37fb-4f94-e3d3-5a3d0bcac881"
      },
      "source": [
        "tesla_rdd.take(5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date,Open,High,Low,Close,AdjClose,Volume',\n",
              " '2019-07-15,248.000000,254.419998,244.860001,253.500000,253.500000,11000100',\n",
              " '2019-07-16,249.300003,253.529999,247.929993,252.380005,252.380005,8149000',\n",
              " '2019-07-17,255.669998,258.309998,253.350006,254.860001,254.860001,9764700',\n",
              " '2019-07-18,255.050003,255.750000,251.889999,253.539993,253.539993,4764500']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As92MXnnihW7",
        "outputId": "e164cc44-d7d9-4dc7-c52b-f0ebe0d7d0f9"
      },
      "source": [
        "type(tesla_rdd)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sINrnl-fihW8"
      },
      "source": [
        "csv_rdd = tesla_rdd.map(lambda row: row.split(\",\"))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M5e2FwLihW8",
        "outputId": "fb0b4d86-142d-4984-e84f-0129ff5601d4"
      },
      "source": [
        "csv_rdd.take(5)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Date', 'Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume'],\n",
              " ['2019-07-15',\n",
              "  '248.000000',\n",
              "  '254.419998',\n",
              "  '244.860001',\n",
              "  '253.500000',\n",
              "  '253.500000',\n",
              "  '11000100'],\n",
              " ['2019-07-16',\n",
              "  '249.300003',\n",
              "  '253.529999',\n",
              "  '247.929993',\n",
              "  '252.380005',\n",
              "  '252.380005',\n",
              "  '8149000'],\n",
              " ['2019-07-17',\n",
              "  '255.669998',\n",
              "  '258.309998',\n",
              "  '253.350006',\n",
              "  '254.860001',\n",
              "  '254.860001',\n",
              "  '9764700'],\n",
              " ['2019-07-18',\n",
              "  '255.050003',\n",
              "  '255.750000',\n",
              "  '251.889999',\n",
              "  '253.539993',\n",
              "  '253.539993',\n",
              "  '4764500']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UId8NyaFihW8"
      },
      "source": [
        "google_rdd = sc.textFile(path_to_data + '/GOOG.csv')\n",
        "amazon_rdd = sc.textFile(path_to_data + '/AMZN.csv')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxFxw1SkihW-"
      },
      "source": [
        "header = csv_rdd.first()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAkrCJxihW-"
      },
      "source": [
        "#создание дф и добавление названия столбцов из rdd \n",
        "tesla_df = csv_rdd.filter(lambda row:row != header).toDF(header)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXTayzuVihW-",
        "outputId": "fe4865bf-e6cd-4a94-cb7c-8ea84004b35d"
      },
      "source": [
        "tesla_df.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "|      Date|      Open|      High|       Low|     Close|  AdjClose|  Volume|\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "|2019-07-15|248.000000|254.419998|244.860001|253.500000|253.500000|11000100|\n",
            "|2019-07-16|249.300003|253.529999|247.929993|252.380005|252.380005| 8149000|\n",
            "|2019-07-17|255.669998|258.309998|253.350006|254.860001|254.860001| 9764700|\n",
            "|2019-07-18|255.050003|255.750000|251.889999|253.539993|253.539993| 4764500|\n",
            "|2019-07-19|255.690002|259.959991|254.619995|258.179993|258.179993| 7048400|\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMpn3MR9ihW-"
      },
      "source": [
        "amazon_df  = sqlContext.read.load(path_to_data + '/AMZN.csv', format ='com.databricks.spark.csv', \n",
        "                                 header = 'true', \n",
        "                                 inferSchema = 'true')\n",
        "\n",
        "google_df  = sqlContext.read.load(path_to_data + '/GOOG.csv', format ='com.databricks.spark.csv', \n",
        "                                 header = 'true', \n",
        "                                 inferSchema = 'true')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyYh5G8bihW-",
        "outputId": "6ccd62b7-7100-40d3-c308-70529da2f7b4"
      },
      "source": [
        "amazon_df.show(2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|2019-07-15|2021.400024|2022.900024|2001.550049| 2020.98999| 2020.98999|2981300|\n",
            "|2019-07-16|2010.579956|2026.319946|2001.219971|2009.900024|2009.900024|2618200|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJvBoWNrihW_",
        "outputId": "32eae529-88ff-4bcc-e476-7c70fe404ee3"
      },
      "source": [
        "amazon_df.printSchema()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- AdjClose: double (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PMnyqWSihW_",
        "outputId": "d62c4e28-120a-4690-df64-04eb2ee7faa8"
      },
      "source": [
        "amazon_df.count()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQVAINGkihW_",
        "outputId": "0ae352c6-9e24-4b32-f114-89651ac38b7d"
      },
      "source": [
        "google_df.show(5)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|2019-07-15|1146.859985|1150.819946|1139.400024|1150.339966|1150.339966| 903800|\n",
            "|2019-07-16|     1146.0|1158.579956|     1145.0|1153.579956|1153.579956|1238800|\n",
            "|2019-07-17|1150.969971|1158.359985| 1145.77002|1146.349976|1146.349976|1170000|\n",
            "|2019-07-18| 1141.73999| 1147.60498| 1132.72998|1146.329956|1146.329956|1291300|\n",
            "|2019-07-19|1148.189941|1151.140015|1129.619995|1130.099976|1130.099976|1647200|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gswZ1NPpihW_",
        "outputId": "f1a6735d-d1f4-4cba-c74a-b743d5d2d73e"
      },
      "source": [
        "#более привычное отображение через дф в пандас \n",
        "amazon_df.toPandas().head(5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>AdjClose</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-07-15</td>\n",
              "      <td>2021.400024</td>\n",
              "      <td>2022.900024</td>\n",
              "      <td>2001.550049</td>\n",
              "      <td>2020.989990</td>\n",
              "      <td>2020.989990</td>\n",
              "      <td>2981300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-07-16</td>\n",
              "      <td>2010.579956</td>\n",
              "      <td>2026.319946</td>\n",
              "      <td>2001.219971</td>\n",
              "      <td>2009.900024</td>\n",
              "      <td>2009.900024</td>\n",
              "      <td>2618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-07-17</td>\n",
              "      <td>2007.050049</td>\n",
              "      <td>2012.000000</td>\n",
              "      <td>1992.030029</td>\n",
              "      <td>1992.030029</td>\n",
              "      <td>1992.030029</td>\n",
              "      <td>2558800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-07-18</td>\n",
              "      <td>1980.010010</td>\n",
              "      <td>1987.500000</td>\n",
              "      <td>1951.550049</td>\n",
              "      <td>1977.900024</td>\n",
              "      <td>1977.900024</td>\n",
              "      <td>3504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-07-19</td>\n",
              "      <td>1991.209961</td>\n",
              "      <td>1996.000000</td>\n",
              "      <td>1962.229980</td>\n",
              "      <td>1964.520020</td>\n",
              "      <td>1964.520020</td>\n",
              "      <td>3185600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date         Open         High  ...        Close     AdjClose   Volume\n",
              "0  2019-07-15  2021.400024  2022.900024  ...  2020.989990  2020.989990  2981300\n",
              "1  2019-07-16  2010.579956  2026.319946  ...  2009.900024  2009.900024  2618200\n",
              "2  2019-07-17  2007.050049  2012.000000  ...  1992.030029  1992.030029  2558800\n",
              "3  2019-07-18  1980.010010  1987.500000  ...  1977.900024  1977.900024  3504300\n",
              "4  2019-07-19  1991.209961  1996.000000  ...  1964.520020  1964.520020  3185600\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp3CYnITihXA",
        "outputId": "64d4e7ed-3e0e-465c-c55a-4bb399c601d7"
      },
      "source": [
        "google_df.show(5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|2019-07-15|1146.859985|1150.819946|1139.400024|1150.339966|1150.339966| 903800|\n",
            "|2019-07-16|     1146.0|1158.579956|     1145.0|1153.579956|1153.579956|1238800|\n",
            "|2019-07-17|1150.969971|1158.359985| 1145.77002|1146.349976|1146.349976|1170000|\n",
            "|2019-07-18| 1141.73999| 1147.60498| 1132.72998|1146.329956|1146.329956|1291300|\n",
            "|2019-07-19|1148.189941|1151.140015|1129.619995|1130.099976|1130.099976|1647200|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbRp_rZqihXA",
        "outputId": "c1d41262-4b53-4e2a-d3e9-5bf9accdd85f"
      },
      "source": [
        "google_df.select(year('Date').alias('yr'), 'Close').show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----------+\n",
            "|  yr|      Close|\n",
            "+----+-----------+\n",
            "|2019|1150.339966|\n",
            "|2019|1153.579956|\n",
            "|2019|1146.349976|\n",
            "|2019|1146.329956|\n",
            "|2019|1130.099976|\n",
            "|2019|1138.069946|\n",
            "|2019|1146.209961|\n",
            "|2019|1137.810059|\n",
            "|2019|1132.119995|\n",
            "|2019|1250.410034|\n",
            "|2019|1239.410034|\n",
            "|2019|1225.140015|\n",
            "|2019|1216.680054|\n",
            "|2019| 1209.01001|\n",
            "|2019| 1193.98999|\n",
            "|2019|1152.319946|\n",
            "|2019|1169.949951|\n",
            "|2019| 1173.98999|\n",
            "|2019|1204.800049|\n",
            "|2019| 1188.01001|\n",
            "+----+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQ8d7eaTrs5V",
        "outputId": "0edf6ae3-25e6-4cd1-f2ed-fcac00734d6a"
      },
      "source": [
        "google_df\\\n",
        ".select(year('Date').alias('yr'), 'Close')\\\n",
        ".filter(google_df['close'] > 1200)\\\n",
        ".show(5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----------+\n",
            "|  yr|      Close|\n",
            "+----+-----------+\n",
            "|2019|1250.410034|\n",
            "|2019|1239.410034|\n",
            "|2019|1225.140015|\n",
            "|2019|1216.680054|\n",
            "|2019| 1209.01001|\n",
            "+----+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu3T6nzZihXA",
        "outputId": "a5b71b95-17f7-49a8-9138-11787ad3df30"
      },
      "source": [
        "google_df\\\n",
        ".select(year('Date').alias('yr'), 'Close')\\\n",
        ".groupby('yr')\\\n",
        ".avg('Close').sort('yr').show()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "|  yr|        avg(Close)|\n",
            "+----+------------------+\n",
            "|2019|1245.3833654621849|\n",
            "|2020|1362.8286906865671|\n",
            "+----+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr3t_hTuihXA",
        "outputId": "2757e692-0346-4d62-931e-40f86e58ea59"
      },
      "source": [
        "amazon_df\\\n",
        ".select(year('Date').alias('year'),'Close')\\\n",
        ".groupby('year')\\\n",
        ".min('Close')\\\n",
        ".sort('year')\\\n",
        ".show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----------+\n",
            "|year| min(Close)|\n",
            "+----+-----------+\n",
            "|2019| 1705.51001|\n",
            "|2020|1676.609985|\n",
            "+----+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCCWqx4MihXB",
        "outputId": "7312ae5b-cb7c-4065-d65f-15b29ce062cb"
      },
      "source": [
        "#план запроса - сам запрос не выполняется в этот момент \n",
        "amazon_df\\\n",
        ".select(year('Date').alias('year'),'Close')\\\n",
        ".groupby('year')\\\n",
        ".min('Close')\\\n",
        ".sort('year')\\\n",
        ".explain()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(3) Sort [year#236 ASC NULLS FIRST], true, 0\n",
            "+- Exchange rangepartitioning(year#236 ASC NULLS FIRST, 200), true, [id=#197]\n",
            "   +- *(2) HashAggregate(keys=[year#236], functions=[min(Close#38)])\n",
            "      +- Exchange hashpartitioning(year#236, 200), true, [id=#193]\n",
            "         +- *(1) HashAggregate(keys=[year#236], functions=[partial_min(Close#38)])\n",
            "            +- *(1) Project [year(cast(Date#34 as date)) AS year#236, Close#38]\n",
            "               +- FileScan csv [Date#34,Close#38] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/jovyan/work/AMZN.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date:string,Close:double>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtLCc5xKihXB"
      },
      "source": [
        "Датафреймы можно сложить во временные таблицы и уже работать с ними через SQLContext. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnkbALl8ihXB"
      },
      "source": [
        "amazon_df.registerTempTable(\"amazon_stocks\")\n",
        "google_df.registerTempTable(\"google_stocks\")\n",
        "tesla_df.registerTempTable(\"tesla_stocks\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hty8tAfbihXB",
        "outputId": "2619c0c4-1c2c-4fee-d9ed-f051dcf7e986"
      },
      "source": [
        "tesla_df.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "|      Date|      Open|      High|       Low|     Close|  AdjClose|  Volume|\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "|2019-07-15|248.000000|254.419998|244.860001|253.500000|253.500000|11000100|\n",
            "|2019-07-16|249.300003|253.529999|247.929993|252.380005|252.380005| 8149000|\n",
            "|2019-07-17|255.669998|258.309998|253.350006|254.860001|254.860001| 9764700|\n",
            "|2019-07-18|255.050003|255.750000|251.889999|253.539993|253.539993| 4764500|\n",
            "|2019-07-19|255.690002|259.959991|254.619995|258.179993|258.179993| 7048400|\n",
            "|2019-07-22|258.750000|262.149994|254.190002|255.679993|255.679993| 6842400|\n",
            "|2019-07-23|256.709991|260.480011|254.500000|260.170013|260.170013| 5023100|\n",
            "|2019-07-24|259.170013|266.070007|258.160004|264.880005|264.880005|11072800|\n",
            "|2019-07-25|233.500000|234.500000|225.550003|228.820007|228.820007|22418300|\n",
            "|2019-07-26|226.919998|230.259995|222.250000|228.039993|228.039993|10027700|\n",
            "|2019-07-29|227.089996|235.940002|226.029999|235.770004|235.770004| 9273300|\n",
            "|2019-07-30|232.899994|243.360001|232.179993|242.259995|242.259995| 8109000|\n",
            "|2019-07-31|243.000000|246.679993|236.649994|241.610001|241.610001| 9178200|\n",
            "|2019-08-01|242.649994|244.509995|231.770004|233.850006|233.850006| 8259500|\n",
            "|2019-08-02|231.350006|236.270004|229.229996|234.339996|234.339996| 6136500|\n",
            "|2019-08-05|229.600006|231.369995|225.779999|228.320007|228.320007| 7028300|\n",
            "|2019-08-06|231.880005|232.500000|225.750000|230.750000|230.750000| 5564200|\n",
            "|2019-08-07|226.500000|233.570007|225.800003|233.419998|233.419998| 4776500|\n",
            "|2019-08-08|234.449997|239.800003|232.649994|238.300003|238.300003| 5274300|\n",
            "|2019-08-09|236.050003|238.960007|233.809998|235.009995|235.009995| 3898200|\n",
            "+----------+----------+----------+----------+----------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fkBEF4UihXC",
        "outputId": "879880fe-42c4-4c0c-cddf-5931f7546882"
      },
      "source": [
        "#sql контекст позваоляет делать запросы на sql из ноутбука\n",
        "sqlContext.sql('''\n",
        "SELECT * from GOOGLE_STOCKS LIMIT 5\n",
        "''').show()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|      Date|       Open|       High|        Low|      Close|   AdjClose| Volume|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "|2019-07-15|1146.859985|1150.819946|1139.400024|1150.339966|1150.339966| 903800|\n",
            "|2019-07-16|     1146.0|1158.579956|     1145.0|1153.579956|1153.579956|1238800|\n",
            "|2019-07-17|1150.969971|1158.359985| 1145.77002|1146.349976|1146.349976|1170000|\n",
            "|2019-07-18| 1141.73999| 1147.60498| 1132.72998|1146.329956|1146.329956|1291300|\n",
            "|2019-07-19|1148.189941|1151.140015|1129.619995|1130.099976|1130.099976|1647200|\n",
            "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6nukyj_ihXC",
        "outputId": "9fa053dd-6f93-4e24-8669-1ced08d413cc"
      },
      "source": [
        "sqlContext.sql('''\n",
        "SELECT year(Date) AS YEAR,\n",
        "       month(Date) AS MONTH,\n",
        "       avg(CLOSE)\n",
        "FROM amazon_stocks\n",
        "GROUP BY YEAR,\n",
        "         MONTH\n",
        "ORDER BY YEAR,\n",
        "         MONTH\n",
        "LIMIT 15\n",
        "''').show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+------------------+\n",
            "|YEAR|MONTH|        avg(CLOSE)|\n",
            "+----+-----+------------------+\n",
            "|2019|    7|1964.6846265384618|\n",
            "|2019|    8|1793.6027220909093|\n",
            "|2019|    9|     1799.12099615|\n",
            "|2019|   10|1752.3317498695653|\n",
            "|2019|   11|      1774.2939941|\n",
            "|2019|   12|1785.7728446190476|\n",
            "|2020|    1|1884.2376128571425|\n",
            "|2020|    2|2066.1752672631574|\n",
            "|2020|    3|1872.3104358636365|\n",
            "|2020|    4|2228.7052408571426|\n",
            "|2020|    5|2394.1840209499996|\n",
            "|2020|    6|      2613.5454545|\n",
            "|2020|    7| 3053.100016222222|\n",
            "+----+-----+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hgXPKDdihXC",
        "outputId": "f8a2531c-d60f-41fa-fbfd-6885008ed7c7"
      },
      "source": [
        "sqlContext.sql('''\n",
        "SELECT t.Date,\n",
        "       t.Open,\n",
        "       t.Close,\n",
        "       t.Open-t.Close AS dif\n",
        "FROM google_stocks t\n",
        "WHERE t.Open-t.Close >=2\n",
        "  OR t.Open-t.Close <= -2\n",
        "''').show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+-----------+-------------------+\n",
            "|      Date|       Open|      Close|                dif|\n",
            "+----------+-----------+-----------+-------------------+\n",
            "|2019-07-15|1146.859985|1150.339966|-3.4799809999999525|\n",
            "|2019-07-16|     1146.0|1153.579956| -7.579956000000038|\n",
            "|2019-07-17|1150.969971|1146.349976|  4.619995000000017|\n",
            "|2019-07-18| 1141.73999|1146.329956| -4.589966000000004|\n",
            "|2019-07-19|1148.189941|1130.099976|  18.08996500000012|\n",
            "|2019-07-22|1133.449951|1138.069946| -4.619995000000017|\n",
            "|2019-07-23|     1144.0|1146.209961| -2.209961000000021|\n",
            "|2019-07-24|1131.900024|1137.810059|  -5.91003499999988|\n",
            "|2019-07-25|1137.819946|1132.119995| 5.6999510000000555|\n",
            "|2019-07-26|1224.040039|1250.410034|-26.369995000000017|\n",
            "|2019-07-31|     1223.0|1216.680054|  6.319946000000073|\n",
            "|2019-08-01|1214.030029| 1209.01001| 5.0200190000000475|\n",
            "|2019-08-02| 1200.73999| 1193.98999|               6.75|\n",
            "|2019-08-05|1170.040039|1152.319946| 17.720092999999906|\n",
            "|2019-08-06|1163.310059|1169.949951| -6.639892000000145|\n",
            "|2019-08-07|     1156.0| 1173.98999|-17.989990000000034|\n",
            "|2019-08-08|1182.829956|1204.800049|-21.970092999999906|\n",
            "|2019-08-09| 1197.98999| 1188.01001|  9.979980000000069|\n",
            "|2019-08-12|1179.209961|1174.709961|                4.5|\n",
            "|2019-08-13|1171.459961| 1197.27002| -25.81005899999991|\n",
            "+----------+-----------+-----------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqDCXzo1ihXC",
        "outputId": "8febfce6-b804-419c-e878-48859d84fca2"
      },
      "source": [
        "sqlContext.sql('''\n",
        "SELECT year(date) AS YEAR,\n",
        "       max(adjclose),\n",
        "       min(adjclose)\n",
        "FROM tesla_stocks\n",
        "GROUP BY YEAR\n",
        "''').show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-------------+-------------+\n",
            "|YEAR|max(adjclose)|min(adjclose)|\n",
            "+----+-------------+-------------+\n",
            "|2019|   430.940002|   211.399994|\n",
            "|2020|   994.320007|  1000.900024|\n",
            "+----+-------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4I7SzBuihXD"
      },
      "source": [
        "joinclose_df = sqlContext.sql('''\n",
        "WITH tesla AS\n",
        "  (SELECT year(date) AS YEAR,\n",
        "          max(adjclose) AS max_tesla,\n",
        "          min(adjclose) AS min_tesla\n",
        "   FROM tesla_stocks\n",
        "   GROUP BY YEAR),\n",
        "     google AS\n",
        "  (SELECT year(date) AS YEAR,\n",
        "          max(adjclose) AS max_google,\n",
        "          min(adjclose) AS min_google\n",
        "   FROM google_stocks\n",
        "   GROUP BY YEAR),\n",
        "     amazon AS\n",
        "  (SELECT year(date) AS YEAR,\n",
        "          max(adjclose) AS max_amazon,\n",
        "          min(adjclose) AS min_amazon\n",
        "   FROM amazon_stocks\n",
        "   GROUP BY YEAR)\n",
        "SELECT t.YEAR, \n",
        "  t.max_tesla, t.min_tesla,\n",
        "  a.max_amazon, a.min_amazon, \n",
        "  g.max_google, g.min_google\n",
        "FROM tesla t\n",
        "LEFT JOIN amazon a ON t.year = a.year\n",
        "LEFT JOIN google g ON t.year = g.year\n",
        "''')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9m32HXXihXD",
        "outputId": "c90d8f3a-c61b-4409-c34a-e32852e52f38"
      },
      "source": [
        "joinclose_df.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------+-----------+----------+-----------+-----------+-----------+\n",
            "|YEAR| max_tesla|  min_tesla|max_amazon| min_amazon| max_google| min_google|\n",
            "+----+----------+-----------+----------+-----------+-----------+-----------+\n",
            "|2019|430.940002| 211.399994|2020.98999| 1705.51001|1361.170044|1130.099976|\n",
            "|2020|994.320007|1000.900024|    3200.0|1676.609985| 1541.73999|1056.619995|\n",
            "+----+----------+-----------+----------+-----------+-----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNzUTeNKihXD"
      },
      "source": [
        "joinclose_df.write.format('parquet').save('join_stocks.parquet')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU0mWUE4ihXD"
      },
      "source": [
        "final_df = sqlContext.read.parquet('join_stocks.parquet')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QszgvmKMihXD",
        "outputId": "18f1802b-44ce-4c20-fa2d-55a70dbd3cad"
      },
      "source": [
        "final_df.show()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+----------+-----------+----------+-----------+-----------+-----------+\n",
            "|YEAR| max_tesla|  min_tesla|max_amazon| min_amazon| max_google| min_google|\n",
            "+----+----------+-----------+----------+-----------+-----------+-----------+\n",
            "|2020|994.320007|1000.900024|    3200.0|1676.609985| 1541.73999|1056.619995|\n",
            "|2019|430.940002| 211.399994|2020.98999| 1705.51001|1361.170044|1130.099976|\n",
            "+----+----------+-----------+----------+-----------+-----------+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}